\documentclass[12pt]{article}
\usepackage{fullpage,enumitem,amsmath,amssymb,graphicx}
\usepackage[linewidth=1pt]{mdframed}
\usepackage{hyperref}
\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{tabularx}
\usepackage{textcomp}
\usepackage{lipsum}
\usepackage{pgfplots}
\usepackage{graphicx}
\graphicspath{ {images/} }
%\usepackage{tikz}
\usepackage{forest}
\usepackage{algorithmic}
\usepackage{parskip}
\usepackage{commath}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{color}
\usepackage{bbm}
\usepackage{bm}
\usepackage{multirow}
\usepackage{siunitx}
\usepackage{caption}
\usepackage{hyphenat}{}
\usepackage{qtree}
\usepackage{float}
\newcommand{\qlabelhook}{\framebox}{}
\DeclarePairedDelimiter{\evdel}{\langle}{\rangle}
\newcommand{\ev}{\operatorname{E}\evdel}
\DeclareMathOperator*{\argmin}{arg\,min} % thin space, items underneath in displays
%\DeclareMathOperator*{\min}{min} % items underneath in displays

\usepackage{listings}
\usepackage{color}

\usetikzlibrary{positioning}
\usepackage{tkz-graph}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=mystyle}

\begin{document}

\begin{center}
{\Large Nativeness Modeling Decisions}

\begin{tabular}{rl}
Name: & Shayne Miel \\
Date: & 02/27/2017
\end{tabular}
\end{center}

This document will present the decisions made around the modeling of nativeness
in ELL writing. I will describe the problem and constraints, the data that I'm
using, potential confounds and what I'm doing to mitigate them, the modeling task,
and the architecture of the baseline and the intended approach.

\section*{Problem Statement}
English Language Learners around the world would like to have rapid and reliable
feedback on the quality of their English writing, both to assess their progress
and to point out ways in which they might improve. One of the aspects of writing
that ELL writers struggle with, broadly speaking, is making their writing look
like the kind of writing produced by native English writers. In order to provide
feedback on that aspect, we would like to be able to predict whether a section
of text looks like it was generated by a native English writer. Ideally, we would
like to be able to provide the probability that any given section of text was
generated by a native English writer. This is sort of like an ELL Turing Test -
if an English Language Learner can successfully fool the model into thinking
that their writing is produced by a native writer, then they are doing very well
on this aspect of writing.

\section*{Constraints}
We do not have any labeled data that says explicitly how ``native'' a piece of
writing is. Instead, we'll use the fact that we know some writing is from native
writers and some is from ELL writers as binary labels. Unfortunately, we also
don't have any large datasets that include both native and ELL writing.\footnote{
There may be something available in the Turnitin database. I have yet to explore
that option.}

In order to have enough data to build a model, we will need to use an amalgamation
of different data sets. I'll describe what we have below. Some of the data sets
I was able to find do have a few prompts with both native and ELL writing. Not enough
to use as a training set, but we can use it as our test set (more on this below).
Some of the data also contains labels at the essay level and the student level
that are somewhat related to writing proficiency (TOEFL levels, CEFR levels, Language
trait scores, etc.). These are not standardized or prevalent enough to want to
use them as labels when training, but it will be useful to look at whether they
correlate with the predicted probability that writing is generated by a native writer.

In order to make an ELL product feasible, it is important that our models be
prompt- and L1-independent, if possible.

\section*{Data}
The following data sets were collected from online sources and data that was
submitted to Turnitin privately. Some are only available
as research corpora so we may need to replace them with other data if we want
to operationalize this model.\footnote{Let's ask a lawyer though.} \ref{table:datasets}
lists the 13 data sets that make up our corpus and \ref{table:L1s} shows the
meanings of the L1 abbreviations used.

\begin {table}
\caption {Datasets}
\label{table:datasets}
\begin{center}
%\newcolumntype{L}{>{\raggedright\arraybackslash}X}
\begin{tabularx}{\textwidth}{l r r X}
Data Set & $n$ essays & $n$ prompts & L1s \\
\hline
\textbf{\textit{ELL}} \\
\hline
\href{http://language.sakura.ne.jp/icnale/}{ICNALE} & $5,600$ & $2$ & CHN, \textbf{ENG}, FIL, HKG, IND, JPN, \newline KOR, PAK, SIN, THA, TWN \\
\href{http://www.comp.nus.edu.sg/~nlp/conll14st.html\#nucle32}{NUCLE} & $1,397$ & $3$ & CHN \\
\href{http://ilexir.co.uk/datasets/index.html}{FCE} & $2,481$ & $44$ & CAT, CHN, FRA, GER, GRC, ITA, \newline JPN, KOR, NL, POL, PRT, RUS, \newline SPA, SWE, THA, TUR \\
\href{https://meta-toolkit.org/data/2016-01-26/ceeaus.tar.gz}{CEEAUS} & $1,008$ & $2$ & CHN, \textbf{ENG}, JPN \\
\href{http://www.u-sacred-heart.ac.jp/okugiri/links/moecs/links/data/data.html}{MOECS} & $199$ & $1$ & \textbf{ENG}, JPN \\
\href{http://koreanlearnercorpusblog.blogspot.be/p/corpus.html}{Gachon} & $15,831$ & $20$ & KOR \\
\href{http://www.bfsu-corpus.org/static/corpora/TECCL_Corpus_V1.1.zip}{TECCL} & $9,864$ & $???$ & CHN \\
Chungdahm & $550$ & $1$ & KOR \\
NOG & $4,694$ & $4$ & CHN \\
\href{https://catalog.ldc.upenn.edu/LDC2014T06}{TOEFL-11} & $12,100$ & $8$ & ARA, CHN, FRA, GER, IND, ITA, \newline JPN, KOR, SPA, TEL, TUR \\
\hline
\textbf{\textit{Native English}} \\
\hline
RA-2016 & $41,227$ & $53$ & \textbf{ENG} \\
SBAC-Field & $29,559$ & $21$ & \textbf{ENG} \\
\href{https://www.kaggle.com/c/asap-aes/data}{ASAP} & $17,677$ & $8$ & \textbf{ENG} \\
\end{tabularx}
\end{center}
\end{table}

\begin {table}
\caption {L1s}
\label{table:L1s}
\begin{center}
%\newcolumntype{L}{>{\raggedright\arraybackslash}X}
\begin{tabularx}{\textwidth}{l l l l l l }
Abbreviation & Language \\
\hline
ARA & Arabic &    GRC  & Greek   &          PRT  & Portuguese \\
BUL & Bulgarian &      HKG  &  Hong Kong Cantonese       &    RUS  & Russian \\
CAT & Catalan &    IND  & Indian languages   &           SIN  & Singapore languages \\
CHN & Chinese &    ITA  & Italian        &   SPA  & Spanish \\
CZE & Czech &     JPN  & Japanese         &     SWE  & Swedish \\
ENG & English &    KOR  & Korean         &   TEL  & Telugu \\
FIL & Filipino &       NL  &  Dutch      &       THA &  Thai \\
FIN & Finnish &    NOR &  Norwegian       &      TSW  & Tswana \\
FRA & French &     PAK  & Urdu         &     TUR  & Turkish \\
GER & German &     POL  & Polish    &        TWN  & Taiwanese
\end{tabularx}
\end{center}
\end{table}

In this collection there are a large number of prompts and L1s represented, with
an approximately even split between native and non-native L1s. Three data sets
(ICNALE, CEEAUS, and MOECS) have both native and non-native writing on the same
prompt.

\section*{Possible Confounds}
There are all sorts of things that could trick our models into learning a correlate
of non-native writing besides writing proficiency. I'll list them in \ref{table:confounds}
and talk about the ways that I've tried to mitigate the risk.

\begin {table}
\caption {Confounds}
\label{table:confounds}
\begin{center}
%\newcolumntype{L}{>{\raggedright\arraybackslash}X}
\begin{tabularx}{\textwidth}{X | X}
Potential Confound & Mitigation \\
\hline
Formatting differences may cause the model to learn that an entire sub-dataset
is native or ELL. &
\begin{enumerate}
    \item Use entirely separate data sets for train and test splits.
    \item Restrict the formatting so that there is no repeated whitespace and
    all characters fall between \textbackslash x32 (Space) and \textbackslash x126
    (\texttildelow).
\end{enumerate} \\
\hline
Prompt differences may cause the model to learn that an entire prompt is native
or ELL. & \begin{enumerate}
    \item Use entirely separate prompts for train, dev and test splits.
    \item Make sure the prompts we do have that contain both ELL and native writing
    are present in the test set.
    \item Control for the contribution of the prompt in the model (add an explanatory
    variable if using regression, add a secondary objective ala \cite{zhong}
    if using a neural network, etc).
\end{enumerate} \\
\hline
Native writers tend to write longer essays and longer sentences than ELL students.
While ``write more'' is something we would want to encourage, it doesn't provide the
kind of meaningful proficiency feedback we'd want to give. & Rather than using
the full essay or even a particular sentence as an input, I will use sliding
windows of $n=100$ characters and take either the average or the max of the predicted
probabilities.
\end{tabularx}
\end{center}
\end{table}

\section*{Task Description}
Our eventual goal is to be able to give sub-essay feedback on which areas look
the most/least proficient. In the absence of proficiency labels, I will use nativeness
as a proxy. So, the eventual goal becomes to be able to give sub-essay feedback
on which areas look the most/least native.

Since we don't really care about the accuracy of our native essay classifier, I'll
use AUC as the metric of comparison between models. That metric tells us ``for
an essay randomly sampled from the native essays and an essay randomly sampled
from the ELL essays, what's the probability that the native essays is ranked higher?''

This is also a convenient metric because we want to use the ICNALE, CEEAUS, and
MOECS data sets as our test set, and the class distribution between native and ELL
writers in those data sets is heavily imbalanced. See ``The Splits.pdf'' for a
description of the train, dev and test sets.

The models will be trained to predict the binary native/non-native classification,
using native as the positive class. Inputs to the classifier will be sliding windows
of characters, such that every document $d \in D$ has $k_d$ windows of $n$
contiguous characters. If I have
enough time, I will try three scenarios for predicting the essay-level label
from the sliding window of characters.

\begin{enumerate}
    \item Treating the windows as separate instances, each with the label of
    the document it came from.

    \item Averaging the window predictions to get the document prediction.

    \item Taking the max window prediction to get the document prediction.
\end{enumerate}

For 1, the loss function is:

$$L(D, Y) = -\frac{1}{K} \sum_{d \in D} \sum_{i=0}^{|d| - n} y_d log(p_{di}) + (1 - y_d) log(1 - p_{di}) $$

where $K = \sum_{d \in D} k_d$, and $p_{di} = f(d[i:i + n])$

For 2 and 3, the loss function is standard log loss:

$$L(D, Y) = -\frac{1}{|D|} \sum_{d \in D}  y_d log(p_d) + (1 - y_d) log(1 - p_d) $$

but we define $p_d$ as either $\frac{1}{|d| - n}\sum_{i=0}^{|d| - n} p_{di}$
\begin{enumerate}
    \item Every window is given the label of the essay from which it came. The
    loss is a standard log loss across all windows:

    $$L(D, Y) = -\frac{1}{K} \sum_{d \in D} \sum_{i=0}^{|d| - n} y_d log(p_{di}) + (1 - y_d) log(1 - p_{di}) $$

    where $K = \sum_{d \in D} k_d$, and $p_{di} = f(d[i:i + n])$
    \item Window predictions are averaged across an essay and the loss is a
    modified version of the loss described above:

    $$L(D, Y) = -\frac{1}{|D|} \sum_{d \in D} y_d log\Big(\frac{1}{|d| - n}\sum_{i=0}^{|d| - n} p_{di}\Big) + (1 - y_d) log\Big(1 - \frac{1}{|d| - n}\sum_{i=0}^{|d| - n} p_{di}\Big) $$

    \item The essay prediction is the minimum window prediciton in that essay
    and the loss is another modified version of the loss described above:

    $$L(D, Y) = -\frac{1}{|D|} \sum_{d \in D} y_d log\Big(min(p_{di} : i \in [0, \dots, |d| - n]\Big) + (1 - y_d) log\Big(1 - min(p_{di} : i \in [0, \dots, |d| - n])\Big) $$

\end{enumerate}

The AUC metric will

\section*{Baselines}
I'll use two baselines: perplexity from a pretrained langauge model,
trained on the billion word corpus

\begin{thebibliography}{1}

\bibitem{zhong}
    Zhong, Yu, and Gil Ettinger.
    "Enlightening Deep Neural Networks with Knowledge of Confounding Factors."
    \emph{arXiv preprint} arXiv:1607.02397 (2016).
\end{thebibliography}

\end{document}


